{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d94248b-c412-4a8d-8192-b40fb996bdd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T08:22:32.367658Z",
     "iopub.status.busy": "2025-04-18T08:22:32.367168Z",
     "iopub.status.idle": "2025-04-18T08:22:33.906732Z",
     "shell.execute_reply": "2025-04-18T08:22:33.905690Z",
     "shell.execute_reply.started": "2025-04-18T08:22:32.367638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities - Yes: 0.0307, No: 0.9692\n",
      "结论：Final Decision: No 概率更高\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Replace with your model path\n",
    "model_path = \"/apdcephfs_qy3/share_1594716/yataobian/yang/output/data/general-verifier\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).cuda()\n",
    "\n",
    "# Example inputs\n",
    "question = \"Where is the capital of China\"\n",
    "ground_truth = \"The captical of China is Bejing, the most big city of the world\"\n",
    "student_answer = \"it is Peking\"\n",
    "\n",
    "# 创建带决策前缀的prompt（确保模型直接预测Yes/No）\n",
    "modified_prompt = (\n",
    "    f\"User: ### Question: {question}\\n\\n\"\n",
    "    f\"### Ground Truth Answer: {ground_truth}\\n\\n\"\n",
    "    f\"### Student Answer: {student_answer}\\n\\n\"\n",
    "    \"For the above question, please verify if the student's answer is equivalent to the ground truth answer.\\n\"\n",
    "    \"Do not solve the question by yourself; just check if the student's answer is equivalent to the ground truth answer.\\n\"\n",
    "    \"If correct, output \\\"Final Decision: Yes\\\". If incorrect, output \\\"Final Decision: No\\\".\\n\"\n",
    "    \"Assistant: Final Decision: \"  # 关键修改：将决策部分置于输入序列末尾\n",
    ")\n",
    "\n",
    "# 分词处理（注意添加return_offsets_mapping用于定位）\n",
    "inputs = tokenizer(modified_prompt, \n",
    "                  return_tensors=\"pt\", \n",
    "                  return_offsets_mapping=True).to(model.device)\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "# 前向推理获取logits\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(input_ids)\n",
    "logits = outputs.logits  # [batch=1, seq_len=56, vocab_size=32000]\n",
    "\n",
    "# 确定预测位置（最后一个token的位置）\n",
    "predict_pos = input_ids.shape[1] - 1  # 对应\"Final Decision: \"后的预测位置\n",
    "\n",
    "# 提取目标位置的logits\n",
    "next_token_logits = logits[0, predict_pos, :]  # [vocab_size]\n",
    "\n",
    "# 获取Yes/No的token ID（考虑分词细节）\n",
    "decision_tokens = tokenizer(\" Yes\", \" No\", add_special_tokens=False) \n",
    "yes_id = decision_tokens.input_ids[0]  # 假设\"Yes\"为单token\n",
    "no_id = decision_tokens.input_ids[1]   # 假设\"No\"为单token\n",
    "\n",
    "# 计算概率分布\n",
    "probs = torch.softmax(next_token_logits, dim=0)\n",
    "yes_prob = probs[yes_id].item()\n",
    "no_prob = probs[no_id].item()\n",
    "\n",
    "# 结果判定\n",
    "print(f\"Probabilities - Yes: {yes_prob:.4f}, No: {no_prob:.4f}\")\n",
    "if yes_prob > no_prob:\n",
    "    print(\"结论：Final Decision: Yes 概率更高\")\n",
    "else:\n",
    "    print(\"结论：Final Decision: No 概率更高\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e32482ae-657a-4f6a-8657-44d6020fb8dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T07:44:27.491024Z",
     "iopub.status.busy": "2025-04-18T07:44:27.490598Z",
     "iopub.status.idle": "2025-04-18T07:44:27.500475Z",
     "shell.execute_reply": "2025-04-18T07:44:27.499897Z",
     "shell.execute_reply.started": "2025-04-18T07:44:27.491006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1474,    25, 16600, 15846,    25, 10967,   374,   279,  6722,   315,\n",
       "          5616,   271, 14374, 25413, 29098, 21806,    25,   576,  6427,   938,\n",
       "           315,  5616,   374,   393, 58852,    11,   279,  1429,  2409,  3283,\n",
       "           315,   279,  1879,   271, 14374, 11726, 21806,    25,   432,   374,\n",
       "           393, 58852,   271,  2461,   279,  3403,  3405,    11,  4486, 10146,\n",
       "           421,   279,  5458,   594,  4226,   374, 13578,   311,   279,  4910,\n",
       "          8046,  4226,   624,  5404,   537, 11625,   279,  3405,   553,  6133,\n",
       "            26,  1101,  1779,   421,   279,  5458,   594,  4226,   374, 13578,\n",
       "           311,   279,  4910,  8046,  4226,   624,  2679,   279,  5458,   594,\n",
       "          4226,   374,  4396,    11,  2550,   330, 19357, 40425,    25,  7414,\n",
       "          3263,  1416,   279,  5458,   594,  4226,   374, 15114,    11,  2550,\n",
       "           330, 19357, 40425,    25,  2308,  3263, 21388,    25, 13023, 40425,\n",
       "            25,  7414]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9ee5c62-0d4c-4477-8642-87a89f735466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T07:47:11.567292Z",
     "iopub.status.busy": "2025-04-18T07:47:11.566797Z",
     "iopub.status.idle": "2025-04-18T07:47:11.572256Z",
     "shell.execute_reply": "2025-04-18T07:47:11.571484Z",
     "shell.execute_reply.started": "2025-04-18T07:47:11.567269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(7414)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76f92b89-d015-4f4d-9281-b252ed44721b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T07:43:44.757673Z",
     "iopub.status.busy": "2025-04-18T07:43:44.757175Z",
     "iopub.status.idle": "2025-04-18T07:43:44.764096Z",
     "shell.execute_reply": "2025-04-18T07:43:44.763546Z",
     "shell.execute_reply.started": "2025-04-18T07:43:44.757645Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_implication(self, answer1, answer2, question, *args, **kwargs):\n",
    "    # 创建prompt模板（与原始实现一致）\n",
    "    prompt = (\n",
    "        f\"User: ### Question: {question}\\n\\n\"\n",
    "        f\"### Ground Truth Answer: {answer1}\\n\\n\"\n",
    "        f\"### Student Answer: {answer2}\\n\\n\"\n",
    "        \"For the above question, please verify if the student's answer is equivalent to the ground truth answer.\\n\"\n",
    "        \"Do not solve the question by yourself; just check if the student's answer is equivalent to the ground truth answer.\\n\"\n",
    "        \"If the student's answer is correct, output \\\"Final Decision: Yes\\\". If the student's answer is incorrect, output \\\"Final Decision: No\\\". Assistant: Final Decision: \"\n",
    "    )\n",
    "\n",
    "    # Tokenize输入（需要添加return_offsets_mapping获取位置信息）\n",
    "    inputs = self.tokenizer(prompt, return_tensors=\"pt\", return_offsets_mapping=True).to(self.model.device)\n",
    "    \n",
    "    # 前向传播获取logits\n",
    "    with torch.no_grad():\n",
    "        outputs = self.model.forward(**inputs)\n",
    "    \n",
    "    # 提取最后一个token位置（即\"Final Decision: \"后的位置）\n",
    "    last_token_idx = inputs.input_ids.shape[-1] - 1\n",
    "    next_token_logits = outputs.logits[0, last_token_idx, :]  # [batch, seq_len, vocab]\n",
    "    \n",
    "    # 获取\"Yes\"和\"No\"的token ID\n",
    "    yes_token = self.tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n",
    "    no_token = self.tokenizer.encode(\"No\", add_special_tokens=False)[0]\n",
    "    \n",
    "    # 计算概率分布\n",
    "    probs = torch.softmax(next_token_logits, dim=-1)\n",
    "    yes_prob = probs[yes_token].item()\n",
    "    no_prob = probs[no_token].item()\n",
    "    \n",
    "    # 判断概率高低\n",
    "    return \"Final Decision: Yes\" if yes_prob > no_prob else \"Final Decision: No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8c375-3122-4ea3-b390-bf08572f7b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
